{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus - Triplets part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "\n",
    "pd.set_option('display.max_columns', 100) # to display all columns at all time\n",
    "pd.options.mode.chained_assignment = None # to ignore false-positive warnings about chained assignments\n",
    "data = pd.read_csv('ElectionsData.csv', header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT!!!\n",
    "We need to convert everything to something that is not 'category' so either float or int.\n",
    "\n",
    "This is because we can't use train_test_split on anything that is category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_categories(attr):\n",
    "    return len(data[attr].astype('category').cat.categories)\n",
    "\n",
    "obj_attr = [(col, count_categories(col))  for col in data if data[col].dtype==np.object]\n",
    "\n",
    "# XXXX REMOVE THIS. THIS IS ONLY BECAUSE OF THE ASSUMPTION THAT AT THIS POINT WE DON'T HAVE NaN's\n",
    "data = data.dropna()\n",
    "\n",
    "# Handle binary columns (Gender, Married, etc.)\n",
    "for attr,cnt in obj_attr:\n",
    "        data[attr] = data[attr].astype('category')\n",
    "\n",
    "data['Gender_Int'] = data['Gender'].map({'Female':0, 'Male':1}).astype(int)\n",
    "data['Voting_Time_Int'] = data['Voting_Time'].map({'By_16:00':0, 'After_16:00':1}).astype(int)\n",
    "\n",
    "data = data.drop(['Gender','Voting_Time'],axis=1)\n",
    "\n",
    "for attr in ['Married','Looking_at_poles_results','Financial_agenda_matters','Will_vote_only_large_party']:\n",
    "    data[attr+'_Int'] = data[attr].map({'No':0, 'Yes':1}).astype(int)\n",
    "    data = data.drop(attr,axis=1)\n",
    "\n",
    "# Handle categorical columns and add one-hot vectors\n",
    "for attr in ['Most_Important_Issue','Main_transportation','Occupation']:\n",
    "    data = pd.concat([data, pd.get_dummies(data[attr],prefix=attr)], axis=1)\n",
    "    data = data.drop(attr,axis=1)\n",
    "    \n",
    "# For convenience, we want 'Vote_Int' to be at the beginning\n",
    "for attr,cnt in obj_attr:\n",
    "    if attr=='Vote':\n",
    "        data[attr] = data[attr].astype('category').cat.rename_categories(range(1,cnt+1)).astype('float')\n",
    "\n",
    "data['Age_group_Int'] = data['Age_group'].map({'Below_30':0, '30-45':1, '45_and_up':2}).astype(int)\n",
    "data = data.drop(['Age_group'],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relief Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def find_closest(data,index,row):\n",
    "    nearhit_dist = nearmiss_dist = None\n",
    "    nearhit = nearmiss = None\n",
    "\n",
    "    for idx, cur_row in data.iterrows():\n",
    "        if idx == index:\n",
    "            continue\n",
    "        cur_vote = cur_row.Vote\n",
    "        dist = sum([(row[c]-cur_row[c])**2 for c in data if c != 'Vote'])\n",
    "        if cur_vote == row.Vote:\n",
    "            if nearhit_dist is None or dist < nearhit_dist:\n",
    "                nearhit_dist = dist\n",
    "                nearhit = cur_row\n",
    "        else:\n",
    "            if nearmiss_dist is None or dist < nearmiss_dist:\n",
    "                nearmiss_dist = dist\n",
    "                nearmiss = cur_row\n",
    "    return nearhit, nearmiss\n",
    "        \n",
    "\n",
    "def relief(data, samples=0.2,tau=0):\n",
    "    weights = {}\n",
    "    #initialize the weights\n",
    "    for f in data.columns.values:\n",
    "        if f == 'Vote':\n",
    "            continue\n",
    "        weights[f] = 0\n",
    "    #go over the samples\n",
    "    i = 0\n",
    "    for index, row in data.sample(frac=samples).iterrows():\n",
    "        i = i + 1\n",
    "        print \"i =\", i\n",
    "        vote = row.Vote\n",
    "        #find nearest from class, and its index\n",
    "        #find nearest from outside class\n",
    "        nearhit, nearmiss = find_closest(data,index,row)\n",
    "        for f in data.columns.values:\n",
    "            if f == 'Vote':\n",
    "                continue\n",
    "            #weights[f] = weights[f] + (xi-nearmiss(xi))^2 - (xi-nearhit(xi))^2\n",
    "            weights[f] = weights[f] + (row[f] - nearmiss[f])**2 - (row[f] - nearhit[f])**2\n",
    "        attrs = [attr for attr, w in weights.iteritems() if w>tau]\n",
    "        print len(attrs)\n",
    "    return attrs\n",
    "    \n",
    "attrs = relief(data.dropna(),0.001)\n",
    "attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(attrs)\n",
    "open_set = set(attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## SFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "def _sfs(df,learning_model,S=None,cur_max=0):\n",
    "    if S is None:\n",
    "        S = set(['Vote'])\n",
    "    #print S\n",
    "    max_feature = None\n",
    "    for col in [c for c in data if c not in S]:\n",
    "        S.add(col)\n",
    "        noNaN = df[list(S)].dropna()\n",
    "        train_data_X_noNaN = noNaN.drop(['Vote'], axis=1).values\n",
    "        train_data_Y_noNaN = noNaN.Vote.values\n",
    "        # Prepare train and test data using cross validation\n",
    "        X_train_noNaN, X_test_noNaN, Y_train_noNaN, Y_test_noNaN = train_test_split(train_data_X_noNaN, \n",
    "                                                                                train_data_Y_noNaN)\n",
    "        clf = learning_model.fit(X_train_noNaN, Y_train_noNaN)\n",
    "        Y_pred_noNaN = clf.predict(X_test_noNaN)\n",
    "        tmp = metrics.accuracy_score(Y_test_noNaN, Y_pred_noNaN)\n",
    "        if(tmp > cur_max):\n",
    "            cur_max = tmp\n",
    "            max_feature = col\n",
    "        S.remove(col)\n",
    "    if (max_feature is not None):\n",
    "        S.add(max_feature)\n",
    "        #print cur_max\n",
    "        S, cur_max = _sfs(df,learning_model,S,cur_max)\n",
    "    return S, cur_max\n",
    "\n",
    "\n",
    "#df: dataframe, d-dimensional feature-set\n",
    "#learning_model: classifier by which to measure predictive power, higher = better\n",
    "#iterations: this is to allow restarts because the learning_model may be random. picks the best subset out of all\n",
    "def sfs(df,learning_model,iterations=1):\n",
    "    best_score = 0\n",
    "    best_subset = set(['Vote'])\n",
    "    #because the function is susceptible to local maximums,\n",
    "    #we will run it with random restarts and take the best subset\n",
    "    for i in range(iterations):\n",
    "        S, score = _sfs(df,learning_model)\n",
    "        if(score > best_score):\n",
    "            best_score = score\n",
    "            best_subset = S\n",
    "    best_subset.remove('Vote')\n",
    "    return best_subset, best_score\n",
    "\n",
    "# Example usage 1\n",
    "forest = RandomForestClassifier(n_estimators = 15)\n",
    "S, accuracy = sfs(data,forest,5)\n",
    "print S, accuracy\n",
    "\n",
    "# Example usage 2\n",
    "svm = SVC()\n",
    "S, accuracy = sfs(data,svm)\n",
    "print S, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "def _bds(df,learning_model,sfs_open=None):\n",
    "    if open_set is None:\n",
    "        sfs_open = set(df.columns.values)\n",
    "        sfs_open.remove('Vote')\n",
    "    sbs_open = set.copy(sfs_open)\n",
    "    \n",
    "    S_sfs = set()\n",
    "    S_sbs = set.copy(sfs_open)\n",
    "    \n",
    "    S_max = None\n",
    "    \n",
    "    cur_max_forward = 0\n",
    "    cur_max_backward = 0\n",
    "    cur_max = 0\n",
    "    \n",
    "    train_data_Y_noNaN = df.dropna().Vote.values\n",
    "    \n",
    "    while S_sfs != S_sbs:\n",
    "        \n",
    "        max_feature = None\n",
    "        min_feature = None\n",
    "        changed = False\n",
    "        \n",
    "        print \"#### FORWARD ####\"\n",
    "        print S_sfs\n",
    "        #forward selection\n",
    "        #find a feature in sfs_open and add it\n",
    "        for f in sfs_open:\n",
    "            S_sfs.add(f)\n",
    "            noNaN = df[list(S_sfs)].dropna()\n",
    "            \n",
    "            train_data_X_noNaN = noNaN.values\n",
    "            \n",
    "            # Prepare train and test data using cross validation\n",
    "            X_train_noNaN, X_test_noNaN, Y_train_noNaN, Y_test_noNaN = train_test_split(train_data_X_noNaN, \n",
    "                                                                                    train_data_Y_noNaN)\n",
    "            clf = learning_model.fit(X_train_noNaN, Y_train_noNaN)\n",
    "            Y_pred_noNaN = clf.predict(X_test_noNaN)\n",
    "            tmp = metrics.accuracy_score(Y_test_noNaN, Y_pred_noNaN)\n",
    "            \n",
    "            #pick the best feature to add\n",
    "            if max_feature is None:\n",
    "                max_feature = f\n",
    "            if tmp > cur_max_forward:\n",
    "                cur_max_forward = tmp\n",
    "                max_feature = f\n",
    "            if tmp > cur_max:\n",
    "                cur_max = tmp\n",
    "                S_max = S_sfs\n",
    "            S_sfs.remove(f)\n",
    "        if (max_feature is not None):\n",
    "            S_sfs.add(max_feature)\n",
    "            #sbs can't remove feature selected by sfs\n",
    "            sbs_open.remove(max_feature)\n",
    "            sfs_open.remove(max_feature)\n",
    "            \n",
    "        print \"#### BACKWARD ####\"\n",
    "        print S_sbs\n",
    "        #backward selection\n",
    "        for f in sbs_open:\n",
    "            S_sbs.remove(f)\n",
    "            noNaN = df[list(S_sbs)].dropna()\n",
    "            \n",
    "            train_data_X_noNaN = noNaN.values\n",
    "            \n",
    "            # Prepare train and test data using cross validation\n",
    "            X_train_noNaN, X_test_noNaN, Y_train_noNaN, Y_test_noNaN = train_test_split(train_data_X_noNaN, \n",
    "                                                                                    train_data_Y_noNaN)\n",
    "            clf = learning_model.fit(X_train_noNaN, Y_train_noNaN)\n",
    "            Y_pred_noNaN = clf.predict(X_test_noNaN)\n",
    "            tmp = metrics.accuracy_score(Y_test_noNaN, Y_pred_noNaN)\n",
    "            \n",
    "            #find least damaging feature to remove\n",
    "            if min_feature is None:\n",
    "                min_feature = f\n",
    "            if(tmp > cur_max_backward):\n",
    "                cur_max_backward = tmp\n",
    "                min_feature = f\n",
    "            if tmp > cur_max:\n",
    "                cur_max = tmp\n",
    "                S_max = S_sfs\n",
    "            S_sbs.add(f)\n",
    "        if (min_feature is not None):\n",
    "            S_sbs.remove(min_feature)\n",
    "            sfs_open.remove(min_feature)\n",
    "            sbs_open.remove(min_feature)\n",
    "    return S_max, cur_max\n",
    "\n",
    "\n",
    "#df: dataframe\n",
    "#learning_model: classifier by which to measure predictive power, higher = better\n",
    "#iterations: this is to allow restarts because the learning_model may be random. picks the best subset out of all\n",
    "def bds(df,learning_model,iterations=1,open_set=None):\n",
    "    best_score = 0\n",
    "    best_subset = None\n",
    "    #because the function is susceptible to local maximums,\n",
    "    #we will run it with random restarts and take the best subset\n",
    "    for i in range(iterations):\n",
    "        S, score = _bds(df,learning_model,open_set)\n",
    "        if(score > best_score):\n",
    "            best_score = score\n",
    "            best_subset = S\n",
    "    return best_subset, best_score\n",
    "\n",
    "# Example usage 1\n",
    "forest = RandomForestClassifier(n_estimators = 15)\n",
    "S, accuracy = bds(data,forest,iterations=1,open_set=open_set)\n",
    "print S, accuracy\n",
    "\n",
    "# Example usage 2\n",
    "#svm = SVC()\n",
    "#S, accuracy = sfs(data,svm)\n",
    "#print S, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
